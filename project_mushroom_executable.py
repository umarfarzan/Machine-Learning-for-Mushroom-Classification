# -*- coding: utf-8 -*-
"""Project_Mushroom finale.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g6e6AJWC9D9aRybsOabMersGNgHQF8TL
"""

!pip install scikit-learn

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

from google.colab import files
uploaded = files.upload()

"""The project that i worked on is focused on the Mushroom Records information dataset, in order to discover patterns, make predictions and obtain insights. the whole objective is to find patterns between attributes and determine whether a mushroom is edible or poisonous as a means of prediction through machine learning models.

"""

df =pd.read_csv('/content/mushroom.csv')
df.head(10) #Return 10 rows of data

"""In order to grasp an understanding of the present data,i carefully examined all of the features"""

df.info()
df.describe()

"""i dropped attributes class e and class p, everything else belongs to x dataframe"""

x = df.drop(['class=e', 'class=p'], axis=1)
x.info()

x.head(10)

"""y stores features class=e and class=p"""

y = df[['class=e', 'class=p']]
y.info()

y.head(10)

"""it would be redundant to have a column where the feature does not exist, so best remove it The attribute veil type= partial was deleted, because it had all values as true meaning it was redundant and could not provide meaningful insight to the data.

"""

zeros_columns = df.columns[df.eq(1).all()]

print("Columns with all 1s:", zeros_columns)

x.drop(['veil-type=p'], axis='columns', inplace=True)

x.head(10)

"""I removed the attribute cap-shape=c because its variance was very close to 0, it could only provide a very narrow understanding of the data.

"""

x.drop(['cap-shape=c'], axis='columns', inplace=True)

"""no null data exists"""

y.isnull().sum()

"""I then removed all columns with variance less than 0.1, as I wouldn’t be able to obtain good insight of the data with them, over 48 columns were stored in x data frame at this point.

"""

variances = df.var()

print(variances)

"""the variances of features that were greater than 0.1 were kept as below that threshold , there wouldnt be much use of the features"""

x_var = df.loc[:, df.var() >= 0.1]

x_var.head(10)

x_var.drop(['class=p','class=e'], axis='columns', inplace=True)

x_var.head(10)

y.head(10)

"""then after the separation of data values i moved on to check for null values, there were no columns with null values so i was good to go"""

null = df.isnull().sum()
print(null)

"""the features were stored in x_miss data frame and these are the columns as you can see with over 48 features and the column names are also printed below

"""

x_miss = x_var.dropna(axis=1)

x_miss.head(10)

df_columns = x_miss.columns
print(df_columns)

"""the first heat map is features against features exclusing the target class (edible or poisonous) to identify relationship between features"""

import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = x_miss.corr()

plt.figure(figsize=(50, 50))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)

plt.show()

"""The second heatpmap combines the features and the target so that i can feasibly see each feature's impact on the target class feature:"""

df_combined = pd.concat([x_miss, y], axis=1)

matrix = df_combined.corr()

plt.figure(figsize=(50, 50))
sns.heatmap(matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap between Features and Target Variable')
plt.show()

"""then i moved on to the scaling the data as this is an essential step in preprocessing which would be necessary in the later stages

"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
xn = sc.fit_transform(x_miss)
print(xn[0:2,]) # top  rows in x

x_miss.head(10)

"""based on the correlation matrix i will select only the features which have a high correlation with the target

```
# This is formatted as code
```

the selected features based on my analysis with the heat map are stored in x_corr
"""

selected_columns = ['odor=n','odor=f','gill-size=b','gill-size=n','gill-color=b','stalk-surface-below-ring=k','stalk-surface-below-ring=k', 'stalk-surface-above-ring=k', 'ring-type=p' ]
x_corr = x_miss[selected_columns].copy()

x_corr.head(10)

"""These are the selected features based on the SelectKBest method

> Indented block


"""

from sklearn.datasets import load_digits
from sklearn.feature_selection import SelectKBest, chi2
selector= SelectKBest(chi2, k=6)
x_new= selector.fit(x_miss, y)

cols_idxs = selector.get_support(indices=True)
x_new = x_miss.iloc[:,cols_idxs]
x_new.head(10)

"""bar plots were plotted to easily visualize the data for significant features


the first bar plot was for feature odor=none
the second bar plot was for odor=foul
the third bar plot was for ring type=p

"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

combined_data = pd.concat([x_var, y], axis=1)

plt.figure(figsize=(10, 7))
sns.countplot(x='odor=n', data=combined_data)
plt.title('Bar Plot for odor=n')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

combined_data = pd.concat([x_var, y], axis=1)

plt.figure(figsize=(10, 7))
sns.countplot(x='odor=f', data=combined_data)
plt.title('Bar Plot for odor=f')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

combined_data = pd.concat([x_var, y], axis=1)

plt.figure(figsize=(10, 7))
sns.countplot(x='ring-type=p', data=combined_data)
plt.title('Bar Plot for ring-type=p')
plt.show()

"""To identify a link between the number of instances where a given feature was true and how important it is with relation to a mushroom being edible or poisonous, I used a bar plot to count the instances with the feature present and absent.

a violin plot shows similar result for odor=n against class=p, showing that when there was no odor the mushrooms likely classified as poisonous
"""

import seaborn as sns
sns.violinplot(x='odor=n', y='class=p', data=df)

"""then i split the test and train data into x and y with a test size of 0.3"""

X_train, X_test, y_train, y_test = \
train_test_split(x_corr, y, test_size=0.3, random_state=1)

y_train.head(10)

"""the first machine learning algorithm i focused on was the decision tree ,
I created a decision tree to visualize the data and obtain insight about the feature importance,
based on my data with features selected based on high correlation.

"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()

clf = clf.fit(X_train, y_train)

from sklearn.tree import export_text
text_representation = export_text(clf)
print(text_representation)

!pip install graphviz
!pip install pydotplus

"""here is the the visualization of the decision tree:


"""

from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus

feature_names = [
    "odor=n", "odor=f",
    "gill-size=b", "gill-size=n",
    "gill-color=b", "stalk-surface-below-ring=k",
    "stalk-surface-below-ring=k", "stalk-surface-above-ring=k",
    "ring-type=p"
]

dot_data = StringIO()
export_graphviz(
    clf,
    out_file=dot_data,
    filled=True,
    rounded=True,
    special_characters=True,
    feature_names=feature_names,
    class_names=['Edible','Poisonous']
)

graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('heart.png')
Image(graph.create_png())

"""Max depth:5
Number of leaves:11
Feature weights: [6.83357330e-01 5.05028756e-02 0.00000000e+00 1.09026568e-01
 0.00000000e+00 8.01503598e-05 0.00000000e+00 1.11008747e-02
 1.45932202e-01]
"""

print(f'Max depth:{clf.get_depth()}')
print(f'Number of leaves:{clf.get_n_leaves()}')
print(f'Feature weights:{clf.feature_importances_}')

feature_importances = clf.feature_importances_

feature_importance_dict = dict(zip(feature_names, feature_importances))

for feature, importance in feature_importance_dict.items():
    print(f"{feature}: {importance}")

"""Observing the analysis of the feature weights , the highest feature score was the first feature odor=n with a feature score of 0.683 indicating that this is the most important feature and the second highest although not very significant feature is the ring-type=p, which was 0.1459
There were features with  a score of 0.0 indicating that they didn’t provide meaningful insight into predicting the data however I did not drop the features as the feature may not have been well captured by the decision tree and may be beneficial in other context, due to high correlation in the heatmap with the target variable

"""

y_pred = clf.predict(X_test)
y_pred

from sklearn.metrics import accuracy_score
print("Train Accuracy :", accuracy_score(y_train, clf.predict(X_train)))
print("Accuracy of test:",accuracy_score(y_test, y_pred))

from sklearn.metrics import multilabel_confusion_matrix

cm = multilabel_confusion_matrix(y_test, y_pred)

for i in range(len(cm)):
    print(f"Confusion matrix for class {i + 1}:\n", cm[i])

"""The confusion matrix clearly indicates the high proportion of True positive (TP) and True Negative (TN) values which indicates the high, accuracy precision and recall for the whole matrix
To confirm this I printed the classification _report which clearly displayed the strong precision, recall and f1 scores

"""

from sklearn.metrics import classification_report

  print(classification_report(y_test,y_pred))

"""Building a neural network to predict the outcome,A neural network comprising of 3 layers, an input layer with 9 neurons, a hidden layer with 16 neurons (Relu Activation) and another hidden layer with 12 neurons (Relu activation) and an output layer with 2 neurons (Softmax activation)
Th model was trained for 100 epochs with a batch size of 64
The loss function used was categorical crossentropy, the adam optimizer was used, and the accuracy was the metric to monitor while training

"""

import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(16, input_dim=9, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(2, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=100, batch_size=64)

pred = np.argmax(y_pred, axis=1)
test = np.argmax(y_test.values, axis=1)

from sklearn.metrics import accuracy_score
a = accuracy_score(pred, test)
print('Accuracy is:', a * 100)

"""After the first few epochs about 5 or 6 the model steeped up to over 97.9% , it then remained constant at this rate, it rapidly learned patterns and relationships in the data, in fact it could be said that it could have reached a point where if I trained further epochs, it would not further improve the model."""

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
#plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

model.summary()

"""i then created a support vector machines Since SVM alggorithm caters to binary classification, the algorithm would be ideal to classify the mushrooms as either edible or poisonous

"""

from sklearn import svm
import numpy as np

y_train_array = y_train.to_numpy()
y_train_labels = np.argmax(y_train_array, axis=1)

clf = svm.SVC()
clf.fit(X_train, y_train_labels)

ypredict = clf.predict(X_test)
ypredict

y_test_array = y_test.to_numpy()
y_test_labels = np.argmax(y_test_array, axis=1)
accuracy = accuracy_score(y_test_labels, ypredict)
class_report = classification_report(y_test_labels, ypredict)
print("acccuracy")
print(accuracy)
print("Classification Report:")
print(class_report)

import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(clf,X_train, y_train_labels, cv=5)

plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Score')
plt.plot(train_sizes, np.mean(val_scores, axis=1), label='Validation Score')
plt.xlabel('Training')
plt.ylabel('Score')
plt.legend(loc='best')
plt.show()

"""SVM’s are known for good generalization performance. In order to evaluate the model’s performance on unseen data, Cross validation is a technique which gives insights on how the model is able to generalize.

i also plotted a graph to compare the training score with the validation score to see if the model was underfitting or overfitting, the model shown respresented a good learning curve
"""

from sklearn.model_selection import cross_val_score
from sklearn import svm

#kernel was set to linear to simplify the model
clf = svm.SVC(kernel='linear')

#cv is the number of folds
cv_scores = cross_val_score(clf, X_train, y_train_labels, cv=5)

print("Scores:", cv_scores)

print("Mean Score:", np.mean(cv_scores))
print("Standard Deviation", np.std(cv_scores))

"""The scores represent the performance of the AI model on different sets of the training data.
Each value shows the accuracy on one-fold of cross validation, with a total of 5 folds.
The mean score is about 0.9766 which shows that the model is highly accurate.
And the standard deviation is about 0.0023 which is a small value and indicates that the model is consistent upon training.

Overall, exploring the Mushroom Records Information dataset allowed me to obtain valuable insights for distinguishing mushrooms for classification, key attributes strongly pertaining to the classification of mushrooms were portrayed visually through the heatmap. Most prominently, the absence of odor proved to be a high indicator of mushroom edibility, whereas foul odor showed high significance to poisonous mushrooms, other features such as gill color being buff and some stalk feature characteristics like the stalk surface above and below ground being silky also correlated with poisonous mushrooms.
The decision tree model, which was made on selected features of high correlation represented the feature importance, and the confusion matrix reaffirmed the recall and precision. The deep learning capabilities of the neural network displayed applaudable accuracy which narrowed down over the epochs. The combined insights from decision tree algorithm, neural networks and SVM algorithm with cross validation provided an accurate approach to classify the mushroom dataset.
"""